{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac4fb0-2111-4106-b866-8f18b238478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install\n",
    "!pip install huggingface_hub # ALl the open source models available in HF\n",
    "!pip install ctransformers # Loading quantize models\n",
    "!pip install langchain #Langchain is a Framework of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0617ce-4e42-4596-ab0f-000c01858bc8",
   "metadata": {},
   "source": [
    "HF link : https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a852dd-f600-4379-93a5-cc2ab98c016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using wget you can download model from HF  ###############You need to replace blob to resolve otherwise it won't download model\n",
    "#https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/blob/main/llama-2-7b-chat.ggmlv3.q3_K_M.bin\n",
    "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_S.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c1530-aae7-4e0f-a468-ecb3e631d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers # for loading quantize models with the help of  Ctransformers and Llamacpp packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224e42f-77fd-4341-98e7-2f03d420a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =\"llama-2-7b-chat.Q4_K_S.gguf\" # you can choose other GGML or GGUF models also like Mistral, llama, wizardllm, etc\n",
    "\n",
    "llm = CTransformers(model=model_path, model_type='llama', # model type should mentioned correctly else it won't work\n",
    "                    config={'max_new_tokens':128, # Output text response will be limited\n",
    "                          'temperature':0.01}) # Temperature is used to control randomness (0 to 1). 1 is for creative response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20dce1-4c55-44f1-8aa1-a1f92e27d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm('AI is going to')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1f82d-1d1c-4a78-9069-67ecfe8d817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc919c-bd46-40b4-a497-51e3b301ffb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bf043ac-1153-42f3-ac85-fbb45ce007e1",
   "metadata": {},
   "source": [
    "LLamacpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099a29a-9c14-46f7-bdf1-1b827fd90fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a5ead-6580-43d1-b49f-8f33113c472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import (\n",
    "    StreamingStdOutCallbackHandler,\n",
    ")  # for streaming resposne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f869e2-7d15-4b32-acdd-0201de09fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU\n",
    "# model_path = \"/content/llama-2-7b-chat.Q8_0.gguf\" #model path\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "     model_path=model_path, callback_manager=callback_manager, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a02557-c705-4e87-8dd4-f42aa9c11603",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"write code in python for calculator?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bb239-32cb-4687-ba76-dba6a8086f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466afdd3-4166-4e40-8f32-0a2a8b39230a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
